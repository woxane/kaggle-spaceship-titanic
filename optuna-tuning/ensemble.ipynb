{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ce4b21-cbee-4ef2-9b1f-e9dafe0bd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3988bdca-f90e-4b37-9ad9-1ec107fa3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('../dataset/train.csv')\n",
    "df = full_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061800c5-35a9-48a7-846e-7c0db9eb372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6055a8e9-baa1-4555-95c5-babafba21b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def update_train_df(dataframe):\n",
    "    imputers = {\n",
    "        'name_imputer': SimpleImputer(strategy='constant', fill_value='Unknown Unknown'),\n",
    "        'num_imputer': SimpleImputer(strategy='median'),\n",
    "        'cat_imputer': SimpleImputer(strategy='constant', fill_value='Unknown'),\n",
    "    }\n",
    "\n",
    "    num_col = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    cat_cols = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # Impute Part\n",
    "    df.loc[:, 'Name'] = imputers['name_imputer'].fit_transform(df.loc[:, 'Name'].to_frame())\n",
    "    df.loc[:, num_col] = imputers['num_imputer'].fit_transform(df.loc[:, num_col])\n",
    "    df.loc[:, cat_cols] = imputers['cat_imputer'].fit_transform(df.loc[:, cat_cols])\n",
    "\n",
    "    # Feature engineering Part\n",
    "    df.loc[:, 'FirstName'] = df.loc[:, 'Name'].apply(lambda x: x.split()[0])\n",
    "    df.loc[:, 'LastName'] = df.loc[:, 'Name'].apply(lambda x: x.split()[1])\n",
    "\n",
    "    df.loc[:, 'PassengerGGGG'] = df.loc[:, 'PassengerId'].apply(lambda x: int(x.split('_')[0]))\n",
    "    df.loc[:, 'PassengerPP'] = df.loc[:, 'PassengerId'].apply(lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    df.loc[:, 'NumberOfFellows'] = df.groupby('PassengerGGGG')['PassengerGGGG'].transform('count')\n",
    "    df.loc[:, 'IsAlone'] = df.loc[:, 'NumberOfFellows'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "    df['cabin_deck'] = df['Cabin'].apply(lambda x: x.split('/')[0] if x != 'Unknown' else 'U')\n",
    "    df['cabin_num'] = df['Cabin'].apply(lambda x: int(x.split('/')[1]) if x != 'Unknown' else -1)\n",
    "    df['cabin_side'] = df['Cabin'].apply(lambda x: x.split('/')[2] if x != 'Unknown' else 'U')\n",
    "\n",
    "    df.loc[:, 'TotalSpend'] = df.loc[:, ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)\n",
    "    df['IsSpendingZero'] = (df['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "    df.loc[:, 'n_family_member_in_group'] = df.groupby(['PassengerGGGG', 'LastName'])['LastName'].transform('count')\n",
    "        \n",
    "    # Removing unnecessary columns\n",
    "    df.drop(['Cabin', 'Name', 'PassengerId'], inplace=True, axis=1)\n",
    "    df.drop(['FirstName', 'LastName', 'PassengerGGGG'], inplace=True, axis=1)\n",
    "\n",
    "    # Encoding \n",
    "    cat_data = df.select_dtypes('object')\n",
    "    cat_data.loc[:, ['CryoSleep', 'VIP']] = cat_data.loc[:, ['CryoSleep', 'VIP']].astype('str')\n",
    "    \n",
    "    one_hot_encoder = OneHotEncoder(drop='first', dtype=int)\n",
    "    \n",
    "    cat_data_encoded = one_hot_encoder.fit_transform(cat_data)\n",
    "    encoded_feature_names = one_hot_encoder.get_feature_names_out(cat_data.columns)\n",
    "    \n",
    "    cat_encoded = pd.DataFrame(cat_data_encoded.toarray(), columns = encoded_feature_names, index=cat_data.index)\n",
    "    \n",
    "    non_cat = df.drop(cat_data.columns, axis=1)\n",
    "    df = pd.concat([non_cat, cat_encoded], axis=1)\n",
    "    \n",
    "    df.loc[:, 'Transported'] = df.loc[:, 'Transported'].astype(int)\n",
    "    \n",
    "    return df, imputers, one_hot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de17b5a5-555e-4f70-a9e9-5008abbde6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def update_test_df(dataframe, imputers, one_hot_encoder):\n",
    "    num_col = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    cat_cols = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # Impute Part\n",
    "    df.loc[:, 'Name'] = imputers['name_imputer'].transform(df.loc[:, 'Name'].to_frame())\n",
    "    df.loc[:, num_col] = imputers['num_imputer'].transform(df.loc[:, num_col])\n",
    "    df.loc[:, cat_cols] = imputers['cat_imputer'].transform(df.loc[:, cat_cols])\n",
    "\n",
    "    # Feature engineering Part\n",
    "    df.loc[:, 'FirstName'] = df.loc[:, 'Name'].apply(lambda x: x.split()[0])\n",
    "    df.loc[:, 'LastName'] = df.loc[:, 'Name'].apply(lambda x: x.split()[1])\n",
    "\n",
    "    df.loc[:, 'PassengerGGGG'] = df.loc[:, 'PassengerId'].apply(lambda x: int(x.split('_')[0]))\n",
    "    df.loc[:, 'PassengerPP'] = df.loc[:, 'PassengerId'].apply(lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    df.loc[:, 'NumberOfFellows'] = df.groupby('PassengerGGGG')['PassengerGGGG'].transform('count')\n",
    "    df.loc[:, 'IsAlone'] = df.loc[:, 'NumberOfFellows'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "    df['cabin_deck'] = df['Cabin'].apply(lambda x: x.split('/')[0] if x != 'Unknown' else 'U')\n",
    "    df['cabin_num'] = df['Cabin'].apply(lambda x: int(x.split('/')[1]) if x != 'Unknown' else -1)\n",
    "    df['cabin_side'] = df['Cabin'].apply(lambda x: x.split('/')[2] if x != 'Unknown' else 'U')\n",
    "\n",
    "    df.loc[:, 'TotalSpend'] = df.loc[:, ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)\n",
    "    df['IsSpendingZero'] = (df['TotalSpend'] == 0).astype(int)\n",
    "\n",
    "    df.loc[:, 'n_family_member_in_group'] = df.groupby(['PassengerGGGG', 'LastName'])['LastName'].transform('count')\n",
    "    \n",
    "    # Removing unnecessary columns\n",
    "    df.drop(['Cabin', 'Name', 'PassengerId'], inplace=True, axis=1)\n",
    "    df.drop(['FirstName', 'LastName', 'PassengerGGGG'], inplace=True, axis=1)\n",
    "\n",
    "    # Encoding\n",
    "    cat_data = df.select_dtypes('object')\n",
    "    cat_data.loc[:, ['CryoSleep', 'VIP']] = cat_data.loc[:, ['CryoSleep', 'VIP']].astype('str')\n",
    "        \n",
    "    cat_data_encoded = one_hot_encoder.transform(cat_data)\n",
    "    encoded_feature_names = one_hot_encoder.get_feature_names_out(cat_data.columns)\n",
    "    \n",
    "    cat_encoded = pd.DataFrame(cat_data_encoded.toarray(), columns = encoded_feature_names, index=cat_data.index)\n",
    "    \n",
    "    non_cat = df.drop(cat_data.columns, axis=1)\n",
    "    df = pd.concat([non_cat, cat_encoded], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317ef3ce-4800-4291-85a7-e1250833d137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_314742/2274740918.py:59: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 1 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, 'Transported'] = df.loc[:, 'Transported'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "df_train_updated, train_imputers, train_ohe = update_train_df(df_train) \n",
    "\n",
    "df_test_updated = update_test_df(df_test, train_imputers, train_ohe)\n",
    "\n",
    "X_train = df_train_updated.drop('Transported', axis=1)\n",
    "y_train = df_train_updated.loc[:, 'Transported']\n",
    "\n",
    "X_test = df_test_updated.drop('Transported', axis=1)\n",
    "y_test = df_test_updated.loc[:, 'Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c00480-5c3b-414d-b3e3-874309a64b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    VotingClassifier, StackingClassifier,\n",
    "    RandomForestClassifier, BaggingClassifier,\n",
    "    ExtraTreesClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6692eb0-27cb-4e91-9c42-19c02a2f8f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=690,\n",
    "    max_depth=11,\n",
    "    learning_rate=0.027599754249374484,\n",
    "    subsample=0.9116649086760387,\n",
    "    colsample_bytree=0.9721498601228151,\n",
    "    gamma=1.2251505971550578,\n",
    "    reg_alpha=0.9135594878672821,\n",
    "    reg_lambda=0.23339826491201715,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=813,\n",
    "    max_depth=19,\n",
    "    min_samples_split=16,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='log2',\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bag_model = BaggingClassifier(\n",
    "    n_estimators=88,\n",
    "    max_samples=0.6205453147546152,\n",
    "    max_features=0.7264387059650675,\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "et_model = ExtraTreesClassifier(\n",
    "    n_estimators=569,\n",
    "    max_depth=39,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=13,\n",
    "    max_features=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada_model = AdaBoostClassifier(\n",
    "    n_estimators=387,\n",
    "    learning_rate=0.26922125547729353,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    num_leaves=23,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.04750121729656987,\n",
    "    n_estimators=222,\n",
    "    subsample=0.8052084722704039,\n",
    "    colsample_bytree=0.9802854475796334,\n",
    "    reg_alpha=0.45245445482085095,\n",
    "    reg_lambda=0.12909159687822422,\n",
    "    min_child_samples=6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svc_model = SVC(\n",
    "    C=6.5326956537464715,\n",
    "    kernel='poly',\n",
    "    degree=2,\n",
    "    gamma='auto',\n",
    "    coef0=0.629725471561754,\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "calibrated_linsvc = CalibratedClassifierCV(\n",
    "    estimator=LinearSVC(\n",
    "        C=0.9055783406210581,\n",
    "        loss='squared_hinge',\n",
    "        random_state=42,\n",
    "        max_iter=5000\n",
    "    ),\n",
    "    method='sigmoid',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "nu_svc_model = NuSVC(\n",
    "    kernel='rbf',\n",
    "    nu=0.44372674202060064,\n",
    "    gamma='auto',\n",
    "    coef0=0.24019318229491016,\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "calibrated_sgd = CalibratedClassifierCV(\n",
    "    estimator=SGDClassifier(random_state=42),\n",
    "    method='isotonic',\n",
    "    cv=9\n",
    ")\n",
    "\n",
    "logreg_model = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    penalty='l2',\n",
    "    C=0.006499032640858296,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis(\n",
    "    solver='eigen',\n",
    "    shrinkage='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f00465fc-40c6-43e7-8a4f-ca2fbb867a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('rf', rf_model),\n",
    "        ('bag', bag_model),\n",
    "        ('et', et_model),\n",
    "        ('ada', ada_model),\n",
    "        ('lgbm', lgbm_model),\n",
    "        ('svc', make_pipeline(StandardScaler(), svc_model)),\n",
    "        ('nusvc', make_pipeline(StandardScaler(), nu_svc_model)),\n",
    "        ('linsvc', make_pipeline(StandardScaler(), calibrated_linsvc)),\n",
    "        ('cal_sgd', calibrated_sgd),\n",
    "        ('logreg', make_pipeline(StandardScaler(), logreg_model)),\n",
    "        ('lda', make_pipeline(StandardScaler(), lda_model))\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ca5e2ce-46b8-4d54-ba46-87ccda6c5683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3484, number of negative: 3470\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501007 -> initscore=0.004026\n",
      "[LightGBM] [Info] Start training from score 0.004026\n",
      "Soft Voting Accuracy: 0.816561242093157\n"
     ]
    }
   ],
   "source": [
    "voting_soft.fit(X_train, y_train)\n",
    "y_soft = voting_soft.predict(X_test)\n",
    "print(\"Soft Voting Accuracy:\", accuracy_score(y_test, y_soft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ad48b2f-8c1d-4c40-b3d0-3da5ac246b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3484, number of negative: 3470\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501007 -> initscore=0.004026\n",
      "[LightGBM] [Info] Start training from score 0.004026\n",
      "Hard Voting Accuracy: 0.8102357676825762\n"
     ]
    }
   ],
   "source": [
    "voting_hard = VotingClassifier(\n",
    "    estimators=voting_soft.estimators,\n",
    "    voting='hard',\n",
    "    n_jobs=-1\n",
    ")\n",
    "voting_hard.fit(X_train, y_train)\n",
    "y_hard = voting_hard.predict(X_test)\n",
    "print(\"Hard Voting Accuracy:\", accuracy_score(y_test, y_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20040a42-8f00-4cf6-9764-10210be398c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3484, number of negative: 3470\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501007 -> initscore=0.004026\n",
      "[LightGBM] [Info] Start training from score 0.004026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2787, number of negative: 2776\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2788, number of negative: 2776\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2787, number of negative: 2776\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1930\n",
      "[LightGBM] [Info] Number of positive: 2787, number of negative: 2776\n",
      "[LightGBM] [Info] Number of data points in the train set: 5563, number of used features: 32\n",
      "[LightGBM] [Info] Number of positive: 2787, number of negative: 2776\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500989 -> initscore=0.003955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Start training from score 0.003955\n",
      "[LightGBM] [Info] Total Bins 1931\n",
      "[LightGBM] [Info] Number of data points in the train set: 5563, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500989 -> initscore=0.003955\n",
      "[LightGBM] [Info] Start training from score 0.003955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1929\n",
      "[LightGBM] [Info] Number of data points in the train set: 5564, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501078 -> initscore=0.004313\n",
      "[LightGBM] [Info] Start training from score 0.004313\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1930\n",
      "[LightGBM] [Info] Number of data points in the train set: 5563, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500989 -> initscore=0.003955\n",
      "[LightGBM] [Info] Start training from score 0.003955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.247081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1931\n",
      "[LightGBM] [Info] Number of data points in the train set: 5563, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500989 -> initscore=0.003955\n",
      "[LightGBM] [Info] Start training from score 0.003955\n",
      "Stacking Accuracy: 0.81943645773433\n"
     ]
    }
   ],
   "source": [
    "stacking = StackingClassifier(\n",
    "    estimators=voting_soft.estimators,\n",
    "    final_estimator=make_pipeline(StandardScaler(),\n",
    "                                  LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    passthrough=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "stacking.fit(X_train, y_train)\n",
    "y_stack = stacking.predict(X_test)\n",
    "print(\"Stacking Accuracy:\", accuracy_score(y_test, y_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01bf6a5b-43c2-4205-833a-80eb6db1ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_314742/2274740918.py:59: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 1 0 ... 1 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, 'Transported'] = df.loc[:, 'Transported'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "train, train_imputers, train_ohe = update_train_df(full_df)\n",
    "\n",
    "test_df = pd.read_csv('../dataset/test.csv')\n",
    "test = update_test_df(test_df, train_imputers, train_ohe)\n",
    "\n",
    "X = train.drop('Transported', axis=1)\n",
    "y = train.loc[:, 'Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36d98e2a-cd9d-438e-903c-19fc2a38c6cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1933\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n"
     ]
    }
   ],
   "source": [
    "voting_soft.fit(X, y)\n",
    "y_soft = voting_soft.predict(test).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b97d3c53-9bd5-4a61-acff-775e429cc5dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1933\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n"
     ]
    }
   ],
   "source": [
    "voting_hard.fit(X, y)\n",
    "y_hard = voting_hard.predict(test).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36003006-e351-4458-8aa0-cde18aa4fb0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4378, number of negative: 4315\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1933\n",
      "[LightGBM] [Info] Number of data points in the train set: 8693, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503624 -> initscore=0.014495\n",
      "[LightGBM] [Info] Start training from score 0.014495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/khabith/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Number of positive: 3502, number of negative: 3452\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1933\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1929\n",
      "[LightGBM] [Info] Number of positive: 3503, number of negative: 3452\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 32\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 32\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 6955, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503666 -> initscore=0.014666\n",
      "[LightGBM] [Info] Start training from score 0.014666\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1933\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503595 -> initscore=0.014380\n",
      "[LightGBM] [Info] Start training from score 0.014380\n"
     ]
    }
   ],
   "source": [
    "stacking.fit(X, y)\n",
    "y_stack = stacking.predict(test).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f74fcaa3-751f-4ac0-9657-1851715f6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../dataset/test.csv')\n",
    "\n",
    "\n",
    "submission_soft = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Transported' : y_soft.astype(bool)\n",
    "})\n",
    "submission_soft.to_csv('new_ensemble_soft.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03cce1db-df31-413b-ac3c-1ea93e25ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../dataset/test.csv')\n",
    "\n",
    "\n",
    "submission_hard = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Transported' : y_hard.astype(bool)\n",
    "})\n",
    "submission_hard.to_csv('new_ensemble_host.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ea894b0-a977-4a43-a2d9-f9b2459fa413",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../dataset/test.csv')\n",
    "\n",
    "\n",
    "submission_stacking = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Transported' : y_stack.astype(bool)\n",
    "})\n",
    "submission_stacking.to_csv('new_ensemble_stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60d026-dd87-4298-88bb-9c2402340937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
